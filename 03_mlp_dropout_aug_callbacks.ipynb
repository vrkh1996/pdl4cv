{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert\">\n",
    "<div style=\"direction:ltr;text-align:center;font-family: B Tahoma; font-size:24pt\"> Practical Deep Learning Course for Computer Vision\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert\">\n",
    "<div style=\"direction:ltr;text-align:left;font-family:Tahoma; font-size:16pt\"> Dropout, Data Augmention, Callbacks,...\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/overfitting.png\" alt=\"Overfitting\">\n",
    "<a href=\"https://miro.medium.com/max/2400/1*UCd6KrmBxpzUpWt3bnoKEA.png\">Reference</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;text-align:left;font-family: Tahoma\">\n",
    "Import the required libraries.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "de829a92-1fb6-44ad-a2c6-fc1001e1f6e1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;text-align:left;font-family: Tahoma\">\n",
    "Load the dataset:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;text-align:left;font-family: Tahoma\">\n",
    "Displaying the dataset:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d745529588>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;text-align:left;font-family: Tahoma\">\n",
    "Create tf.data:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_image, label):\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    label = tf.one_hot(label,  depth=10)\n",
    "    label = tf.squeeze(label)\n",
    "    input_image = input_image / 255\n",
    "    input_image = tf.reshape(input_image, [32*32*3])\n",
    "    \n",
    "    return input_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(128).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;text-align:left;font-family: Tahoma\">\n",
    "Defining the classifier:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1024, activation='relu', input_shape=(32*32*3,)), \n",
    "                             tf.keras.layers.Dense(1024, activation='relu'), \n",
    "                             tf.keras.layers.Dense(512, activation='relu'), \n",
    "                             tf.keras.layers.Dense(256, activation='relu'), \n",
    "                             tf.keras.layers.Dense(10, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1024)              3146752   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 4,855,050\n",
      "Trainable params: 4,855,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;text-align:left;font-family: Tahoma\">\n",
    "Fitting the classifier:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1563 steps\n",
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.8921 - accuracy: 0.3085\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 1.7202 - accuracy: 0.3791\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.6452 - accuracy: 0.4084\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.5931 - accuracy: 0.4275\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.5409 - accuracy: 0.4445\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.5033 - accuracy: 0.4589\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.4622 - accuracy: 0.4733\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.4378 - accuracy: 0.4819\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.4080 - accuracy: 0.4919\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3786 - accuracy: 0.5031\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3576 - accuracy: 0.5104\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3297 - accuracy: 0.5209\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3029 - accuracy: 0.5274\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2724 - accuracy: 0.5356\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2524 - accuracy: 0.5449\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2249 - accuracy: 0.5550\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2034 - accuracy: 0.5597\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1699 - accuracy: 0.5732\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1523 - accuracy: 0.5787\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1259 - accuracy: 0.5877\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;text-align:left;font-family: Tahoma\">\n",
    "Evaluating the classifier:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 54.59 %\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(train_dataset, verbose=0)\n",
    "print(\"Accuracy is %.2f %%\" %(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 45.92 %\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "print(\"Accuracy is %.2f %%\" %(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/dropout.png\" alt=\"Dropout\">\n",
    "<a href=\"https://www.researchgate.net/profile/Amine_Ben_khalifa/publication/309206911/figure/fig3/AS:418379505651712@1476760855735/Dropout-neural-network-model-a-is-a-standard-neural-network-b-is-the-same-network.png\">Reference</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:ltr;text-align:left;font-family: Tahoma\">\n",
    "Adding Dropout, Data Augmention, Callback,...:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(input_image, label):\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    label = tf.one_hot(label,  depth=10)\n",
    "    label = tf.squeeze(label)\n",
    "    input_image = tf.image.random_flip_left_right(input_image)\n",
    "    input_image = tf.image.resize_with_crop_or_pad(input_image, 32, 32)\n",
    "    input_image = input_image / 255\n",
    "    input_image = tf.reshape(input_image, [32*32*3])\n",
    "    \n",
    "    return input_image, label\n",
    "\n",
    "def preprocess_test(input_image, label):\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    label = tf.one_hot(label,  depth=10)\n",
    "    label = tf.squeeze(label)\n",
    "    input_image = input_image / 255\n",
    "    input_image = tf.reshape(input_image, [32*32*3])\n",
    "    \n",
    "    return input_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.map(preprocess_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(128).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.map(preprocess_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(512, activation='relu', input_shape=(32*32*3,)), \n",
    "                             tf.keras.layers.Dense(512, activation='relu'), \n",
    "                             tf.keras.layers.Dense(256, activation='relu'), \n",
    "                             tf.keras.layers.Dropout(0.5),\n",
    "                             tf.keras.layers.Dense(128, activation='relu'), \n",
    "                             tf.keras.layers.Dropout(0.5),\n",
    "                             tf.keras.layers.Dense(10, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 2,001,546\n",
      "Trainable params: 2,001,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', \n",
    "                                                                          tf.keras.metrics.TopKCategoricalAccuracy(k=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, monitor=\"val_accuracy\"),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                                       save_best_only=True, \n",
    "                                       monitor=\"val_accuracy\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1563 steps, validate for 313 steps\n",
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 2.0619 - accuracy: 0.2224 - top_k_categorical_accuracy: 0.7492 - val_loss: 1.8528 - val_accuracy: 0.3186 - val_top_k_categorical_accuracy: 0.8284\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.8951 - accuracy: 0.3077 - top_k_categorical_accuracy: 0.8203 - val_loss: 1.8239 - val_accuracy: 0.3393 - val_top_k_categorical_accuracy: 0.8396\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.8406 - accuracy: 0.3352 - top_k_categorical_accuracy: 0.8361 - val_loss: 1.7807 - val_accuracy: 0.3546 - val_top_k_categorical_accuracy: 0.8482\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.8115 - accuracy: 0.3485 - top_k_categorical_accuracy: 0.8401 - val_loss: 1.7929 - val_accuracy: 0.3368 - val_top_k_categorical_accuracy: 0.8499\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.7837 - accuracy: 0.3601 - top_k_categorical_accuracy: 0.8484 - val_loss: 1.7574 - val_accuracy: 0.3616 - val_top_k_categorical_accuracy: 0.8573\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.7693 - accuracy: 0.3643 - top_k_categorical_accuracy: 0.8532 - val_loss: 1.7220 - val_accuracy: 0.3871 - val_top_k_categorical_accuracy: 0.8644\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.7605 - accuracy: 0.3692 - top_k_categorical_accuracy: 0.8547 - val_loss: 1.6886 - val_accuracy: 0.3928 - val_top_k_categorical_accuracy: 0.8739\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.7475 - accuracy: 0.3741 - top_k_categorical_accuracy: 0.8588 - val_loss: 1.6778 - val_accuracy: 0.3996 - val_top_k_categorical_accuracy: 0.8749\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.7436 - accuracy: 0.3764 - top_k_categorical_accuracy: 0.8595 - val_loss: 1.6725 - val_accuracy: 0.3965 - val_top_k_categorical_accuracy: 0.8801\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.7313 - accuracy: 0.3819 - top_k_categorical_accuracy: 0.8638 - val_loss: 1.6915 - val_accuracy: 0.3888 - val_top_k_categorical_accuracy: 0.8704\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.7234 - accuracy: 0.3831 - top_k_categorical_accuracy: 0.8660 - val_loss: 1.6485 - val_accuracy: 0.4051 - val_top_k_categorical_accuracy: 0.8828\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.7116 - accuracy: 0.3905 - top_k_categorical_accuracy: 0.8676 - val_loss: 1.6550 - val_accuracy: 0.4021 - val_top_k_categorical_accuracy: 0.8784\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.7023 - accuracy: 0.3932 - top_k_categorical_accuracy: 0.8689 - val_loss: 1.6506 - val_accuracy: 0.4061 - val_top_k_categorical_accuracy: 0.8807\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.6951 - accuracy: 0.3961 - top_k_categorical_accuracy: 0.8720 - val_loss: 1.6431 - val_accuracy: 0.4120 - val_top_k_categorical_accuracy: 0.8815\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.6903 - accuracy: 0.3988 - top_k_categorical_accuracy: 0.8721 - val_loss: 1.6495 - val_accuracy: 0.4029 - val_top_k_categorical_accuracy: 0.8799\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.6846 - accuracy: 0.3992 - top_k_categorical_accuracy: 0.8740 - val_loss: 1.6454 - val_accuracy: 0.4089 - val_top_k_categorical_accuracy: 0.8805\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.6773 - accuracy: 0.4040 - top_k_categorical_accuracy: 0.8757 - val_loss: 1.6411 - val_accuracy: 0.4155 - val_top_k_categorical_accuracy: 0.8801\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.6780 - accuracy: 0.4024 - top_k_categorical_accuracy: 0.8759 - val_loss: 1.6388 - val_accuracy: 0.4080 - val_top_k_categorical_accuracy: 0.8832\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.6697 - accuracy: 0.4027 - top_k_categorical_accuracy: 0.8775 - val_loss: 1.6486 - val_accuracy: 0.4032 - val_top_k_categorical_accuracy: 0.8780\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.6704 - accuracy: 0.4060 - top_k_categorical_accuracy: 0.8756 - val_loss: 1.6546 - val_accuracy: 0.4030 - val_top_k_categorical_accuracy: 0.8785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d76263b3c8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=20, callbacks=my_callbacks, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 41.12 %\n",
      "Top 5 accuracy is 88.62 %\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, top5 = model.evaluate(train_dataset, verbose=0)\n",
    "print(\"Accuracy is %.2f %%\" %(accuracy * 100))\n",
    "print(\"Top 5 accuracy is %.2f %%\" %(top5 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 40.30 %\n",
      "Top 5 accuracy is 87.85 %\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, top5 = model.evaluate(test_dataset, verbose=0)\n",
    "print(\"Accuracy is %.2f %%\" %(accuracy * 100))\n",
    "print(\"Top 5 accuracy is %.2f %%\" %(top5 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert\">\n",
    "<div style=\"direction:ltr;text-align:left;font-family:B Tahoma\"> Practical Deep Learning Course for Computer Vision\n",
    "<br>Vahid Reza Khazaie<br>\n",
    "</div>\n",
    "<a href=\"https://www.linkedin.com/in/vahidrezakhazaie/\">LinkedIn</a> - <a href=\"https://github.com/vrkh1996\">GitHub</a>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nbpresent": {
   "slides": {
    "300ee14f-a043-486e-b274-7ff253907cd7": {
     "id": "300ee14f-a043-486e-b274-7ff253907cd7",
     "prev": "cb74e0bc-4513-4d13-b7f1-14c3078a7927",
     "regions": {
      "26dc3f39-a230-447c-af4c-f5e5b2fb7835": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c58440a5-3f8f-4f37-9c79-6bf766209406",
        "part": "whole"
       },
       "id": "26dc3f39-a230-447c-af4c-f5e5b2fb7835"
      }
     }
    },
    "878aa53a-1444-4100-8f50-7a408191c579": {
     "id": "878aa53a-1444-4100-8f50-7a408191c579",
     "prev": null,
     "regions": {
      "a6c6843a-5ea6-4fbc-b890-3b4b8ae475b3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "588ee1fa-64b5-453b-ade7-8e6b2515821c",
        "part": "whole"
       },
       "id": "a6c6843a-5ea6-4fbc-b890-3b4b8ae475b3"
      }
     }
    },
    "92e1449b-15f5-40ac-89eb-9496a06af0b0": {
     "id": "92e1449b-15f5-40ac-89eb-9496a06af0b0",
     "prev": "300ee14f-a043-486e-b274-7ff253907cd7",
     "regions": {
      "ea2d28ea-4686-4b1c-832c-836c35a7524e": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "ea2d28ea-4686-4b1c-832c-836c35a7524e"
      }
     }
    },
    "96ffe88e-7b50-43de-afdd-942e564f4e3e": {
     "id": "96ffe88e-7b50-43de-afdd-942e564f4e3e",
     "prev": "878aa53a-1444-4100-8f50-7a408191c579",
     "regions": {
      "b7e52e12-489a-468d-b10c-af2024fd2856": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "de829a92-1fb6-44ad-a2c6-fc1001e1f6e1",
        "part": "whole"
       },
       "id": "b7e52e12-489a-468d-b10c-af2024fd2856"
      }
     }
    },
    "cb74e0bc-4513-4d13-b7f1-14c3078a7927": {
     "id": "cb74e0bc-4513-4d13-b7f1-14c3078a7927",
     "prev": "96ffe88e-7b50-43de-afdd-942e564f4e3e",
     "regions": {
      "444878ee-68f3-4abb-acff-a7079b21e86d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "25f3f538-1ee8-4d98-a6bb-14cbeb7a702d",
        "part": "whole"
       },
       "id": "444878ee-68f3-4abb-acff-a7079b21e86d"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
